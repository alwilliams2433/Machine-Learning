{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Implementation with Unigram Data Representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "path = \"hw2data_1/reviews_tr.csv\"\n",
    "df_train = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load testing data\n",
    "path2 = \"hw2data_1/reviews_te.csv\"\n",
    "df_test = pd.read_csv(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training example:  1000000\n",
      "   label                                               text\n",
      "0      1  first time here food tastes great good environ...\n",
      "1      0  i have been craving burgers lately so i decide...\n",
      "2      1  i love having a place like this in the neighbo...\n"
     ]
    }
   ],
   "source": [
    "# Check training example format\n",
    "print('Number of training example: ',len(df_train))\n",
    "print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words:  207429\n",
      "The number of words/features to exclude:  191905\n",
      "The number of feature to consider:  15524\n"
     ]
    }
   ],
   "source": [
    "# Feature Pruning\n",
    "vocabulary_dict = {}\n",
    "for text in df_train['text']:\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word not in vocabulary_dict: \n",
    "            vocabulary_dict[word] = 1\n",
    "        else:\n",
    "            vocabulary_dict[word] += 1\n",
    "print('The number of unique words: ', len(vocabulary_dict))\n",
    "\n",
    "low_occurrence_word_set = set()\n",
    "for key, value in vocabulary_dict.items():\n",
    "    if value < 100: low_occurrence_word_set.add(key) # Get rid of words with low occurrence; Can further add 'stop words' here\n",
    "\n",
    "print('The number of words/features to exclude: ',len(low_occurrence_word_set))\n",
    "print('The number of feature to consider: ', len(vocabulary_dict) - len(low_occurrence_word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Representation: Unigram (bag-of-words);  Disadvantage: lose the order and locality of the words\n",
    "# Use hashmap to compress the data - only remember the feature whose value is not zero; Can better utilized memory and speed up dot product calculation\n",
    "def data_compression(df_train):\n",
    "    list_dict = []  # Contains training examples compressed with hashmap\n",
    "\n",
    "    for index, row in df_train.iterrows():\n",
    "        new_dict = {}\n",
    "        words = row['text'] + \"\"\n",
    "        words = words.split()\n",
    "        for word in words:\n",
    "            if word not in low_occurrence_word_set: # Feature Pruning\n",
    "                if word in new_dict:\n",
    "                    new_dict[word] += 1\n",
    "                else:\n",
    "                    new_dict[word] = 1\n",
    "\n",
    "        if(row['label'] == 1):  # Incorporate the label with the feature vector\n",
    "            new_dict['*label*'] = 1\n",
    "        else: new_dict['*label*'] = -1\n",
    "\n",
    "        new_dict['*const*'] = 1   # Lifting\n",
    "        list_dict.append(new_dict)\n",
    "\n",
    "    return list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 93.14159655570984 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Load features into a dict\n",
    "start_time = time.time()\n",
    "list_dict = data_compression(df_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Online-batch perceptron implementation\n",
    "def train_unigram(list_dict):\n",
    "    w = {} # Store tmp weights\n",
    "    w_ret = {} # Store the average weigths\n",
    "    \n",
    "    # First pass\n",
    "    random.shuffle(list_dict)\n",
    "    for x in list_dict:\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*':\n",
    "                if key in w:\n",
    "                    dot_product += w[key] * x[key]\n",
    "        \n",
    "        if dot_product * label <= 0:\n",
    "            \n",
    "            for key, value in x.items():\n",
    "                if key != '*label*':\n",
    "                    if key in w:\n",
    "                        w[key] += label * x[key]\n",
    "                    else: w[key] = label * x[key]\n",
    "    \n",
    "    \n",
    "    count = 0    # debug\n",
    "    start_time = time.time()    # debug\n",
    "\n",
    "    # Second pass\n",
    "    random.shuffle(list_dict)\n",
    "    w_ret = dict(w) # Initilize w_ret\n",
    "    for x in list_dict:\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*':\n",
    "                if key in w:\n",
    "                    dot_product += w[key] * x[key]\n",
    "           \n",
    "        if dot_product * label <= 0:\n",
    "            for key, value in x.items():\n",
    "                if key != '*label*':\n",
    "                    if key in w:\n",
    "                        w[key] += label * x[key]\n",
    "                    else: w[key] = label * x[key]\n",
    "        \n",
    "        # Update w_ret\n",
    "        for key, value in w.items():\n",
    "            if key in w_ret:\n",
    "                w_ret[key] += value\n",
    "            else:\n",
    "                w_ret[key] = value\n",
    "        \n",
    "        # For debug purpose\n",
    "        if count % 10000 == 0:    \n",
    "            print('Current progress: ', count, end= ' ')     \n",
    "            print(' time elapsed: ', time.time() - start_time)    \n",
    "        count += 1    \n",
    "    \n",
    "    # Calculate weighted weight vector\n",
    "    length = len(list_dict) + 1\n",
    "    for key, value in w_ret.items():\n",
    "        w_ret[key] /= length\n",
    "    return w_ret\n",
    "    \n",
    "#     # Transfrom back to a vector\n",
    "#     w_vector = [0] * (len(vocabulary_dict) + 1)\n",
    "#     for key, value in w.items():\n",
    "#         if key in vocabulary_dict:\n",
    "#             w_vector[vocabulary_dict[key]] = value\n",
    "#     w_vector[-1] = w['*const*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test weight vector w on the given testing set\n",
    "def test_unigram(df_test, w):\n",
    "    dict_list_test = data_compression(df_test) # Don't really have to load this every time being called\n",
    "    count = 0\n",
    "    wrong = 0\n",
    "    for dictionary in dict_list_test:\n",
    "        count += 1\n",
    "        dot_product = 0\n",
    "        label = dictionary['*label*']\n",
    "        for key, value in dictionary.items():\n",
    "            if key in w and key != '*label*':\n",
    "                dot_product += w[key] * dictionary[key]\n",
    "        if dot_product * label <= 0: wrong += 1 \n",
    "    return (count - wrong) / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the performance of the classifier with different training sizes\n",
    "# start = 1, end = 5 <=> training size range: 10%, 20%,...,50%\n",
    "# list_dict: contains the entire training samples\n",
    "# Return two lists\n",
    "def test(list_dict, start = 1, end = 5):\n",
    "    w_list = []\n",
    "    accuracy_list = []\n",
    "    for i in range(start, end + 1):\n",
    "        list_dict_current_size = list_dict[:int(len(list_dict) * i * 0.1)]\n",
    "        w = train_unigram(list_dict_current_size)\n",
    "        w_list.append(w)\n",
    "        print('Training size: ', i * 10, '%', end='')\n",
    "        \n",
    "        accuracy = test_unigram(df_test, w)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('  Accuracy: ', accuracy)\n",
    "        print()\n",
    "    return w_list, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current progress:  0  time elapsed:  0.08100724220275879\n",
      "Current progress:  10000  time elapsed:  23.473536252975464\n",
      "Current progress:  20000  time elapsed:  47.14367890357971\n",
      "Current progress:  30000  time elapsed:  71.18328142166138\n",
      "Current progress:  40000  time elapsed:  95.21503829956055\n",
      "Current progress:  50000  time elapsed:  119.33020496368408\n",
      "Current progress:  60000  time elapsed:  143.57458090782166\n",
      "Current progress:  70000  time elapsed:  168.09141063690186\n",
      "Current progress:  80000  time elapsed:  192.85489916801453\n",
      "Current progress:  90000  time elapsed:  217.46832156181335\n",
      "Training size:  10 %Accuracy:  0.8841379224170784\n",
      "Current progress:  0  time elapsed:  0.15601396560668945\n",
      "Current progress:  10000  time elapsed:  23.64736580848694\n",
      "Current progress:  20000  time elapsed:  47.74582624435425\n",
      "Current progress:  30000  time elapsed:  71.96625995635986\n",
      "Current progress:  40000  time elapsed:  96.21390128135681\n",
      "Current progress:  50000  time elapsed:  121.00344204902649\n",
      "Current progress:  60000  time elapsed:  145.35024046897888\n",
      "Current progress:  70000  time elapsed:  169.59041619300842\n",
      "Current progress:  80000  time elapsed:  193.89780020713806\n",
      "Current progress:  90000  time elapsed:  218.2229835987091\n",
      "Current progress:  100000  time elapsed:  242.6073715686798\n",
      "Current progress:  110000  time elapsed:  267.0167624950409\n",
      "Current progress:  120000  time elapsed:  291.4889602661133\n",
      "Current progress:  130000  time elapsed:  316.0007176399231\n",
      "Current progress:  140000  time elapsed:  340.6669006347656\n",
      "Current progress:  150000  time elapsed:  365.37649035453796\n",
      "Current progress:  160000  time elapsed:  389.8662450313568\n",
      "Current progress:  170000  time elapsed:  414.3920032978058\n",
      "Current progress:  180000  time elapsed:  438.9907693862915\n",
      "Current progress:  190000  time elapsed:  463.5533311367035\n",
      "Training size:  20 %Accuracy:  0.8888611216973529\n",
      "Current progress:  0  time elapsed:  0.2340223789215088\n",
      "Current progress:  10000  time elapsed:  23.98090648651123\n",
      "Current progress:  20000  time elapsed:  48.128032207489014\n",
      "Current progress:  30000  time elapsed:  72.53179335594177\n",
      "Current progress:  40000  time elapsed:  96.8811342716217\n",
      "Current progress:  50000  time elapsed:  121.36348867416382\n",
      "Current progress:  60000  time elapsed:  146.10387659072876\n",
      "Current progress:  70000  time elapsed:  170.92907547950745\n",
      "Current progress:  80000  time elapsed:  195.97190713882446\n",
      "Current progress:  90000  time elapsed:  220.77949571609497\n",
      "Current progress:  100000  time elapsed:  245.5876808166504\n",
      "Current progress:  110000  time elapsed:  270.4504725933075\n",
      "Current progress:  120000  time elapsed:  295.296861410141\n",
      "Current progress:  130000  time elapsed:  320.2128572463989\n",
      "Current progress:  140000  time elapsed:  345.1768608093262\n",
      "Current progress:  150000  time elapsed:  370.14786195755005\n",
      "Current progress:  160000  time elapsed:  395.30349564552307\n",
      "Current progress:  170000  time elapsed:  420.5927493572235\n",
      "Current progress:  180000  time elapsed:  445.88660073280334\n",
      "Current progress:  190000  time elapsed:  470.8243987560272\n",
      "Current progress:  200000  time elapsed:  495.7918002605438\n",
      "Current progress:  210000  time elapsed:  520.7297983169556\n",
      "Current progress:  220000  time elapsed:  545.9430410861969\n",
      "Current progress:  230000  time elapsed:  570.8824391365051\n",
      "Current progress:  240000  time elapsed:  595.8322381973267\n",
      "Current progress:  250000  time elapsed:  620.8416438102722\n",
      "Current progress:  260000  time elapsed:  646.1852941513062\n",
      "Current progress:  270000  time elapsed:  671.159095287323\n",
      "Current progress:  280000  time elapsed:  696.1432976722717\n",
      "Current progress:  290000  time elapsed:  721.3279347419739\n",
      "Training size:  30 %Accuracy:  0.8900263024721825\n",
      "Current progress:  0  time elapsed:  0.3558349609375\n",
      "Current progress:  10000  time elapsed:  23.942918300628662\n",
      "Current progress:  20000  time elapsed:  47.758028984069824\n",
      "Current progress:  30000  time elapsed:  71.7631561756134\n",
      "Current progress:  40000  time elapsed:  96.13932466506958\n",
      "Current progress:  50000  time elapsed:  120.57190442085266\n",
      "Current progress:  60000  time elapsed:  145.07308721542358\n",
      "Current progress:  70000  time elapsed:  169.540860414505\n",
      "Current progress:  80000  time elapsed:  194.34607195854187\n",
      "Current progress:  90000  time elapsed:  219.07247638702393\n",
      "Current progress:  100000  time elapsed:  243.69065952301025\n",
      "Current progress:  110000  time elapsed:  268.3392460346222\n",
      "Current progress:  120000  time elapsed:  292.8722128868103\n",
      "Current progress:  130000  time elapsed:  317.720632314682\n",
      "Current progress:  140000  time elapsed:  342.8032784461975\n",
      "Current progress:  150000  time elapsed:  367.80932235717773\n",
      "Current progress:  160000  time elapsed:  392.5111155509949\n",
      "Current progress:  170000  time elapsed:  417.2549138069153\n",
      "Current progress:  180000  time elapsed:  442.272554397583\n",
      "Current progress:  190000  time elapsed:  467.585244178772\n",
      "Current progress:  200000  time elapsed:  492.55988025665283\n",
      "Current progress:  210000  time elapsed:  517.2616677284241\n",
      "Current progress:  220000  time elapsed:  541.968649148941\n",
      "Current progress:  230000  time elapsed:  567.0758874416351\n",
      "Current progress:  240000  time elapsed:  591.7874791622162\n",
      "Current progress:  250000  time elapsed:  616.6636984348297\n",
      "Current progress:  260000  time elapsed:  641.4637017250061\n",
      "Current progress:  270000  time elapsed:  666.0474696159363\n",
      "Current progress:  280000  time elapsed:  690.5874304771423\n",
      "Current progress:  290000  time elapsed:  715.2442080974579\n",
      "Current progress:  300000  time elapsed:  739.8699851036072\n",
      "Current progress:  310000  time elapsed:  765.3390872478485\n",
      "Current progress:  320000  time elapsed:  789.908650636673\n",
      "Current progress:  330000  time elapsed:  814.4294085502625\n",
      "Current progress:  340000  time elapsed:  839.0559823513031\n",
      "Current progress:  350000  time elapsed:  863.5683388710022\n",
      "Current progress:  360000  time elapsed:  888.0966973304749\n",
      "Current progress:  370000  time elapsed:  913.0845375061035\n",
      "Current progress:  380000  time elapsed:  937.8171269893646\n",
      "Current progress:  390000  time elapsed:  962.3590886592865\n",
      "Training size:  40 %Accuracy:  0.8923316735494593\n",
      "Current progress:  0  time elapsed:  0.42484164237976074\n",
      "Current progress:  10000  time elapsed:  24.54358458518982\n",
      "Current progress:  20000  time elapsed:  48.5985004901886\n",
      "Current progress:  30000  time elapsed:  73.09127402305603\n",
      "Current progress:  40000  time elapsed:  97.70885920524597\n",
      "Current progress:  50000  time elapsed:  122.63429045677185\n",
      "Current progress:  60000  time elapsed:  147.7681393623352\n",
      "Current progress:  70000  time elapsed:  172.62254190444946\n",
      "Current progress:  80000  time elapsed:  197.4475371837616\n",
      "Current progress:  90000  time elapsed:  222.56322050094604\n",
      "Current progress:  100000  time elapsed:  247.8266158103943\n",
      "Current progress:  110000  time elapsed:  272.6701669692993\n",
      "Current progress:  120000  time elapsed:  297.69486927986145\n",
      "Current progress:  130000  time elapsed:  322.8732123374939\n",
      "Current progress:  140000  time elapsed:  347.9514706134796\n",
      "Current progress:  150000  time elapsed:  372.82343673706055\n",
      "Current progress:  160000  time elapsed:  399.12458539009094\n",
      "Current progress:  170000  time elapsed:  425.3566827774048\n",
      "Current progress:  180000  time elapsed:  451.5827341079712\n",
      "Current progress:  190000  time elapsed:  477.04351782798767\n",
      "Current progress:  200000  time elapsed:  502.7229890823364\n",
      "Current progress:  210000  time elapsed:  528.3907215595245\n",
      "Current progress:  220000  time elapsed:  554.0656251907349\n",
      "Current progress:  230000  time elapsed:  579.2510561943054\n",
      "Current progress:  240000  time elapsed:  604.7949504852295\n",
      "Current progress:  250000  time elapsed:  630.080803155899\n",
      "Current progress:  260000  time elapsed:  655.7133004665375\n",
      "Current progress:  270000  time elapsed:  681.0143435001373\n",
      "Current progress:  280000  time elapsed:  706.4580178260803\n",
      "Current progress:  290000  time elapsed:  732.1119163036346\n",
      "Current progress:  300000  time elapsed:  757.1047232151031\n",
      "Current progress:  310000  time elapsed:  782.0577239990234\n",
      "Current progress:  320000  time elapsed:  807.1589460372925\n",
      "Current progress:  330000  time elapsed:  832.2829675674438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current progress:  340000  time elapsed:  857.2533714771271\n",
      "Current progress:  350000  time elapsed:  882.4730141162872\n",
      "Current progress:  360000  time elapsed:  907.5214285850525\n",
      "Current progress:  370000  time elapsed:  932.5658440589905\n",
      "Current progress:  380000  time elapsed:  957.6134569644928\n",
      "Current progress:  390000  time elapsed:  982.5894596576691\n",
      "Current progress:  400000  time elapsed:  1007.6150705814362\n",
      "Current progress:  410000  time elapsed:  1032.5596687793732\n",
      "Current progress:  420000  time elapsed:  1057.5258693695068\n",
      "Current progress:  430000  time elapsed:  1082.494470834732\n",
      "Current progress:  440000  time elapsed:  1107.4612715244293\n",
      "Current progress:  450000  time elapsed:  1132.4354729652405\n",
      "Current progress:  460000  time elapsed:  1157.4132742881775\n",
      "Current progress:  470000  time elapsed:  1182.725130558014\n",
      "Current progress:  480000  time elapsed:  1207.6961317062378\n",
      "Current progress:  490000  time elapsed:  1232.6615319252014\n",
      "Training size:  50 %Accuracy:  0.8928408544242508\n"
     ]
    }
   ],
   "source": [
    "w_list = []\n",
    "accuracy_list = []\n",
    "w_list, accuracy_list = test(list_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this block for a specific training size test\n",
    "k = 1 # training size percentage e.g. 0.5 <=> 50%\n",
    "list_dict_tmp = list_dict[:int(len(list_dict) * k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this block for a specific training size test\n",
    "start_time = time.time()\n",
    "w = train_unigram(list_dict_tmp) # Test on a smaller subset first\n",
    "print(\"Total Traing Time: --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8949900350491375\n",
      "Testing Time: --- 36.29430174827576 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Run this block for a specific training size test\n",
    "start_time = time.time()\n",
    "print(test_unigram(df_test, w))\n",
    "print(\"Testing Time: --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('perfection', 160.95294404705595),\n",
       " ('gem', 158.07886392113608),\n",
       " ('disappoint', 128.60505139494862),\n",
       " ('phenomenal', 126.74939825060174),\n",
       " ('heavenly', 124.75710024289975),\n",
       " ('heaven', 121.54008545991454),\n",
       " ('incredible', 119.96416003583997),\n",
       " ('perfect', 119.41774258225742),\n",
       " ('skeptical', 118.16856983143016),\n",
       " ('exceeded', 118.12423987576013),\n",
       " ('superb', 117.36995663004338),\n",
       " ('fantastic', 114.17922082077918),\n",
       " ('perfectly', 111.8997281002719),\n",
       " ('minor', 108.97752402247598),\n",
       " ('excellent', 107.68669631330368),\n",
       " ('deliciousness', 107.006903993096),\n",
       " ('hooked', 105.51402948597051),\n",
       " ('delicious', 103.88030211969787),\n",
       " ('worried', 102.35585264414736),\n",
       " ('glad', 100.17325482674518)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOP20 Positive Words when the model has the highest accuracy\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -201.71768028231972),\n",
       " ('flavorless', -194.4140695859304),\n",
       " ('mediocre', -179.92884907115092),\n",
       " ('poisoning', -177.98037401962597),\n",
       " ('underwhelming', -174.85432814567184),\n",
       " ('lacked', -174.53114246885752),\n",
       " ('inedible', -165.3230466769533),\n",
       " ('underwhelmed', -162.6978103021897),\n",
       " ('disgusting', -161.50284749715252),\n",
       " ('tasteless', -159.24835875164123),\n",
       " ('awful', -156.24327175672823),\n",
       " ('meh', -152.08522991477008),\n",
       " ('disappointing', -147.44352455647544),\n",
       " ('horrible', -147.27376672623328),\n",
       " ('potential', -146.4134005865994),\n",
       " ('hopes', -145.04209095790904),\n",
       " ('lukewarm', -143.1106488893511),\n",
       " ('ruined', -142.01343798656202),\n",
       " ('downhill', -140.9140740859259),\n",
       " ('bland', -139.99704000296)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOP20 Negative Words when the model has the highest accuracy\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    Training Size:   5 %  Accuracy:  0.8784088566234123\n",
    "    Training Size:  10 %  Accuracy:  0.8841379224170784\n",
    "    Training Size:  20 %  Accuracy:  0.8888611216973529\n",
    "    Training Size:  30 %  Accuracy:  0.8900263024721825\n",
    "    Training Size:  40 %  Accuracy:  0.8923316735494593\n",
    "    Training Size:  50 %  Accuracy:  0.8928408544242508  Training Time: 1260.2 seconds (21 minutes)\n",
    "    Training Size:  70 %  Accuracy:  0.8937936161838299\n",
    "    Training Size:  80 %  Accuracy:  0.8943902637119598\n",
    "    Training Size:  90 %  Accuracy:  0.8946386486598453\n",
    "    Training Size: 100 %  Accuracy:  0.8949900350491375  Training Time: 2609.9 seconds (43 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8784088566234123\n",
      "--- 36.10690975189209 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# TEST5: size 5%, with pruning\n",
    "start_time = time.time()\n",
    "print(test_unigram(df_test, w))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -138.55044899102018),\n",
       " ('bland', -118.90682186356273),\n",
       " ('mediocre', -111.46507069858603),\n",
       " ('ok', -97.55382892342153),\n",
       " ('overpriced', -94.72186556268875),\n",
       " ('nothing', -92.32591348173037),\n",
       " ('unfortunately', -90.94836103277935),\n",
       " ('horrible', -87.00023999520009),\n",
       " ('okay', -84.86636267274655),\n",
       " ('meh', -80.92920141597168),\n",
       " ('dry', -77.91660166796665),\n",
       " ('disappointment', -77.07839843203136),\n",
       " ('terrible', -76.55296894062118),\n",
       " ('rude', -73.45003099938002),\n",
       " ('guess', -72.98470030599388)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST5: size 5%, with pruning\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delicious', 106.87676246475071),\n",
       " ('amazing', 105.24183516329673),\n",
       " ('perfect', 95.7785244295114),\n",
       " ('perfectly', 93.05735885282294),\n",
       " ('excellent', 89.25439491210176),\n",
       " ('loved', 87.13433731325374),\n",
       " ('wonderful', 85.55646887062258),\n",
       " ('fantastic', 81.12157756844863),\n",
       " ('glad', 77.1717565648687),\n",
       " ('awesome', 73.11065778684426),\n",
       " ('perfection', 71.79282414351712),\n",
       " ('yummy', 65.77378452430952),\n",
       " ('attentive', 65.19891602167957),\n",
       " ('best', 62.77302453950921),\n",
       " ('incredible', 61.44093118137637)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST5: size 5%, with pruning\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8937936161838299\n",
      "--- 36.06127738952637 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# TEST4: size 70%, with pruning, 30 minutes\n",
    "start_time = time.time()\n",
    "print(test_unigram(df_test, w))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -192.93664437622232),\n",
       " ('flavorless', -178.62880338742374),\n",
       " ('lacked', -178.1516912118697),\n",
       " ('mediocre', -173.96528004959993),\n",
       " ('tasteless', -169.71325183821165),\n",
       " ('underwhelmed', -155.58706058991345),\n",
       " ('awful', -152.3856751633212),\n",
       " ('disappointing', -150.11517412117982),\n",
       " ('meh', -148.56916204405422),\n",
       " ('horrible', -144.41585940591514),\n",
       " ('inedible', -143.13583694880435),\n",
       " ('bland', -142.55834348808074),\n",
       " ('underwhelming', -138.57177204032567),\n",
       " ('poisoning', -138.01465855048778),\n",
       " ('hopes', -135.53183781166027)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST4: size 70%, with pruning\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -192.93664437622232),\n",
       " ('flavorless', -178.62880338742374),\n",
       " ('lacked', -178.1516912118697),\n",
       " ('mediocre', -173.96528004959993),\n",
       " ('tasteless', -169.71325183821165),\n",
       " ('underwhelmed', -155.58706058991345),\n",
       " ('awful', -152.3856751633212),\n",
       " ('disappointing', -150.11517412117982),\n",
       " ('meh', -148.56916204405422),\n",
       " ('horrible', -144.41585940591514),\n",
       " ('inedible', -143.13583694880435),\n",
       " ('bland', -142.55834348808074),\n",
       " ('underwhelming', -138.57177204032567),\n",
       " ('poisoning', -138.01465855048778),\n",
       " ('hopes', -135.53183781166027)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST4: size 70%, with pruning\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8844159414223327\n",
      "--- 38.1008095741272 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# TEST3: size 10%, with pruning, 6 minutes\n",
    "start_time = time.time()\n",
    "print(test_unigram(df_test, w))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -175.8940610593894),\n",
       " ('bland', -144.00270997290028),\n",
       " ('mediocre', -140.89094109058908),\n",
       " ('disappointing', -126.93766062339377),\n",
       " ('unfortunately', -121.96344036559634),\n",
       " ('horrible', -111.5720342796572),\n",
       " ('ok', -110.81521184788153),\n",
       " ('disappointment', -105.75664243357566),\n",
       " ('terrible', -102.9160008399916),\n",
       " ('overpriced', -98.51642483575164),\n",
       " ('okay', -97.59347406525934),\n",
       " ('nothing', -95.94946050539495),\n",
       " ('meh', -95.4869951300487),\n",
       " ('awful', -94.18670813291867),\n",
       " ('lacked', -91.22361776382236)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST3: size 10%, with pruning\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('perfect', 120.92620073799262),\n",
       " ('amazing', 109.47656523434766),\n",
       " ('perfection', 104.6861031389686),\n",
       " ('excellent', 103.27271727282726),\n",
       " ('wonderful', 102.14054859451406),\n",
       " ('delicious', 100.46476535234648),\n",
       " ('fantastic', 98.54290457095429),\n",
       " ('perfectly', 96.6631233687663),\n",
       " ('disappoint', 95.32077679223208),\n",
       " ('incredible', 93.81687183128169),\n",
       " ('outstanding', 84.86997130028699),\n",
       " ('loved', 84.54646453535464),\n",
       " ('glad', 81.13015869841301),\n",
       " ('awesome', 79.28472715272848),\n",
       " ('five', 74.54740452595475)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST3: size 10%, with pruning\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8886018455463854\n",
      "--- 34.6804678440094 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# TEST2:Train size 20%, training time 30 minutes\n",
    "start_time = time.time()\n",
    "print(test_unigram(df_test, w))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -197.37132314338427),\n",
       " ('mediocre', -165.49916750416247),\n",
       " ('bland', -156.6994315028425),\n",
       " ('disappointing', -153.76591117044416),\n",
       " ('awful', -138.2646336768316),\n",
       " ('disappointment', -137.90298048509757),\n",
       " ('meh', -131.87646061769692),\n",
       " ('lacked', -128.48018259908702),\n",
       " ('horrible', -124.95228523857381),\n",
       " ('flavorless', -123.66033169834151),\n",
       " ('tasteless', -123.61867690661546),\n",
       " ('terrible', -115.26302368488157),\n",
       " ('unfortunately', -112.25176374118129),\n",
       " ('lacking', -108.7718161409193),\n",
       " ('ok', -106.65787671061645)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST2:TOP15 negative words\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('perfection', 138.8519957400213),\n",
       " ('incredible', 132.52879235603822),\n",
       " ('disappoint', 116.25043874780626),\n",
       " ('perfect', 114.44366778166109),\n",
       " ('glad', 108.07126964365179),\n",
       " ('perfectly', 105.9279053604732),\n",
       " ('delicious', 101.83353083234584),\n",
       " ('amazing', 100.30448847755761),\n",
       " ('wonderful', 99.43891780541097),\n",
       " ('pleased', 98.85502572487138),\n",
       " ('outstanding', 98.13219933900331),\n",
       " ('gem', 98.09133954330228),\n",
       " ('excellent', 95.6563167184164),\n",
       " ('fantastic', 91.38859305703471),\n",
       " ('heaven', 87.7121864390678)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST2:TOP15 negative words\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8849407413423632\n",
      "--- 46.270057916641235 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Test1: Train size 10%, training time 20 minutes\n",
    "start_time = time.time()\n",
    "print(test_unigram(df_test, w))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -157.89968100318995),\n",
       " ('mediocre', -145.28094719052808),\n",
       " ('bland', -144.95483045169547),\n",
       " ('disappointing', -123.40432595674044),\n",
       " ('horrible', -118.47917520824792),\n",
       " ('terrible', -115.91289087109129),\n",
       " ('unfortunately', -115.58870411295887),\n",
       " ('meh', -113.02520974790252),\n",
       " ('disappointment', -104.37770622293777),\n",
       " ('ok', -103.2990470095299),\n",
       " ('awful', -100.83215167848321),\n",
       " ('overpriced', -94.40989590104098),\n",
       " ('nothing', -92.40821591784082),\n",
       " ('rude', -90.76813231867682),\n",
       " ('overcooked', -87.28211717882822)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST1: TOP15 negative words\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('perfect', 126.96207037929621),\n",
       " ('delicious', 108.69828301716983),\n",
       " ('amazing', 102.34909650903491),\n",
       " ('perfectly', 98.88279117208828),\n",
       " ('fantastic', 98.82754172458276),\n",
       " ('wonderful', 97.86928130718692),\n",
       " ('glad', 96.45740542594574),\n",
       " ('perfection', 95.96735032649673),\n",
       " ('excellent', 95.86153138468616),\n",
       " ('incredible', 90.26750732492675),\n",
       " ('outstanding', 87.08667913320866),\n",
       " ('awesome', 81.49759502404976),\n",
       " ('loved', 80.55550444495555),\n",
       " ('pleased', 79.57246427535725),\n",
       " ('disappoint', 79.01483985160148),\n",
       " ('juicy', 70.88135118648813),\n",
       " ('delish', 65.80271197288027),\n",
       " ('heaven', 65.45958540414595),\n",
       " ('yummy', 64.47285527144729),\n",
       " ('five', 63.20279797202028)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST1: TOP15 Positive words\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF Representation\n",
    "frequency = {}\n",
    "for x in list_dict:\n",
    "    for key, value in x.items():\n",
    "        if key in frequency:\n",
    "            frequency[key] += 1\n",
    "        else:\n",
    "            frequency[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "list_dict1 = list(list_dict) # A shallow copy of list_dict\n",
    "# list_dict1 = copy.deepcopy(list_dict)  #This is a deep copy but takes up a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One pass version\n",
    "def train_unigram(list_dict):\n",
    "    w = {} \n",
    "    \n",
    "    # One pass\n",
    "    random.shuffle(list_dict)\n",
    "    for x in list_dict:\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*':\n",
    "                if key in w:\n",
    "                    dot_product += w[key] * x[key]\n",
    "        \n",
    "        if dot_product * label <= 0:           \n",
    "            for key, value in x.items():\n",
    "                if key != '*label*':\n",
    "                    if key in w:\n",
    "                        w[key] += label * x[key]\n",
    "                    else: w[key] = label * x[key]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in list_dict1:\n",
    "    for key, value in x.items():\n",
    "        if key != '*label*' and key != '*const*':\n",
    "            x[key] = x[key] * math.log((len(list_dict1) / frequency[key]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "1000000   151064\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "w_tfidf = train_unigram(list_dict1, vocabulary_dict)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harman', -89.99999999999999),\n",
       " ('mediocre', -84.22474040199054),\n",
       " ('worst', -80.10731281700849),\n",
       " ('roum6b8yd4ykkugqcxtoug', -77.99999999999999),\n",
       " ('underwhelmed', -77.59042463214872),\n",
       " ('hopes', -71.74741110380765),\n",
       " ('disappointing', -70.60585455721528),\n",
       " ('not', -68.8592132041259),\n",
       " ('bland', -64.50013572257728),\n",
       " ('downhill', -63.45078877872636)]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(w_tfidf.items(), key=lambda x: x[1], reverse = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n00b', 92.83507630617008),\n",
       " ('great', 91.8086860288631),\n",
       " ('delicious', 84.33372264813823),\n",
       " ('ftr', 81.41181741504607),\n",
       " ('jmc', 80.96910013008055),\n",
       " ('and', 78.32414188144158),\n",
       " ('ohfh6alqqq35niebd1exuw', 77.99999999999999),\n",
       " ('perfection', 77.70343425406247),\n",
       " ('amazing', 74.67204708243527),\n",
       " ('zfpcpbzssimrybsg9jxndw', 71.99999999999999)]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(w_tfidf.items(), key=lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency['n00b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_tfidf(df_test, w_tfidf, frequency):\n",
    "    dict_list_test, dict_vocabulary_test = data_compression(df_test)\n",
    "    count = 0\n",
    "    wrong = 0\n",
    "    for x in dict_list_test:\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*' and key != '*const*':\n",
    "                if key in frequency: x[key] = x[key] * math.log((len(dict_list_test) / frequency[key]), 10)\n",
    "        \n",
    "        count += 1\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key in w and key != '*label*':\n",
    "                dot_product += w[key] * x[key]\n",
    "        if dot_product * label <= 0: wrong += 1 \n",
    "    return (count - wrong) / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.872884\n",
      "--- 283.20725202560425 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(test_tfidf(df_test, w_tfidf, frequency))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shallow copy and deep copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list1 = []\n",
    "dict1 = {'a': 1, 'b': 2}\n",
    "dict2 = {'f':1, 'e':2}\n",
    "list1.append(dict1)\n",
    "list1.append(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 3, 'b': 2}, {'f': 1, 'e': 2}]\n",
      "[{'a': 3, 'b': 2}, {'f': 1, 'e': 2}]\n"
     ]
    }
   ],
   "source": [
    "list2 = list(list1)\n",
    "list2[0]['a'] = 3\n",
    "print(list1)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 1, 'b': 2}, {'f': 1, 'e': 2}]\n",
      "[{'a': 3, 'b': 2}, {'f': 1, 'e': 2}]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "list2 = copy.deepcopy(list1)\n",
    "list2[0]['a'] = 3\n",
    "print(list1)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-268161e61baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Training set feature vectors after lifting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Training set labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "def train():\n",
    "    \n",
    "    X = [] # Training set feature vectors after lifting\n",
    "    Y = [] # Training set labels\n",
    "    w = [0] * len(X[0]) #weight vector\n",
    "    w = np.array(w)\n",
    "\n",
    "    # Need to shuffle the data\n",
    "    \n",
    "    # First pass\n",
    "    for i in range(len(X)):\n",
    "        if X[i] @ w * Y[i] <= 0: \n",
    "            w = w + Y[i] * X[i]\n",
    "\n",
    "    w_final = np.array(w)\n",
    "\n",
    "    # Need to shuffle the data\n",
    "\n",
    "    \n",
    "    # Second pass\n",
    "    for i in range(len(X)):\n",
    "        if X[i] @ w * Y[i] <= 0: \n",
    "            w = w + Y[i] * X[i]\n",
    "        w_final += w\n",
    "\n",
    "    w_final /= len(X) + 1\n",
    "    \n",
    "    return w_final\n",
    "\n",
    "def test(x, w):\n",
    "    if x @ w > 0: return 1\n",
    "    if x @ w < 0: return 0\n",
    "    else return -1  #Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem lies in dot product: it takes so much time for two vectors with 110000+ values\n",
    "list1 = [1] * 110000\n",
    "list2 = [i for i in range(110000)]  \n",
    "np.array(list1) @ np.array(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# != compares the value, whereas is not compares if they are the same object\n",
    "dict_test = {'*label*': 1}\n",
    "for key, value in dict_test.items():\n",
    "    if key != '*label*': print(\"haha\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
