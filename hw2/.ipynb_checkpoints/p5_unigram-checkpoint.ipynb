{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Implementation with Unigram Data Representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"hw2data_1/reviews_tr.csv\"\n",
    "df_train = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "   label                                               text\n",
      "0      1  first time here food tastes great good environ...\n",
      "1      0  i have been craving burgers lately so i decide...\n",
      "2      1  i love having a place like this in the neighbo...\n",
      "3      1  i had the morning monte which was delicious i ...\n",
      "4      1  i have this app on my phone that lists the pla...\n",
      "5      1  in n out is great i love their burgers and fri...\n",
      "6      1  a local favourite byob beer wine the restauran...\n",
      "7      1  i had zee most elegant evening here with my bf...\n",
      "8      0  outdated decor slow and unfriendly service pai...\n",
      "9      1  what can i say other than wow we so enjoy our ...\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(df_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words:  207429\n"
     ]
    }
   ],
   "source": [
    "vocabulary_set = set()\n",
    "for text in df_train['text']:\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word not in vocabulary_set: vocabulary_set.add(word)\n",
    "print('The number of unique words: ', len(vocabulary_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Representation: Unigram\n",
    "# Use hashmap to compress the data\n",
    "def data_compression(df_train):\n",
    "    list_dict = []  # Contains training examples compressed with hashmap\n",
    "#     vocabulary_dict = {} # Contains the position to reconstruct the feature vector\n",
    "    for index, row in df_train.iterrows():\n",
    "        new_dict = {}\n",
    "        words = row['text'] + \"\"\n",
    "        words = words.split()\n",
    "        for word in words:\n",
    "#             if word not in vocabulary_dict:\n",
    "#                 vocabulary_dict[word] = 0\n",
    "\n",
    "            if word in new_dict:\n",
    "                new_dict[word] += 1\n",
    "            else:\n",
    "                new_dict[word] = 1\n",
    "\n",
    "        if(row['label'] == 1):  # Attach the label\n",
    "            new_dict['*label*'] = 1\n",
    "        else: new_dict['*label*'] = -1\n",
    "\n",
    "        new_dict['*const*'] = 1   # Lifting\n",
    "        list_dict.append(new_dict)\n",
    "\n",
    "#     count = 0\n",
    "#     for word in vocabulary_dict:\n",
    "#         vocabulary_dict[word] = count\n",
    "#         count += 1\n",
    "    return list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "list_dict = data_compression(df_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Incomplete online-batch perceptron implementation: need to complete\n",
    "\n",
    "def train_unigram(list_dict):\n",
    "    w = {} # Store tmp weights\n",
    "    w_ret = {} # Store the average weigths\n",
    "    \n",
    "    # First pass\n",
    "    random.shuffle(list_dict)\n",
    "    for x in list_dict:\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*':\n",
    "                if key in w:\n",
    "                    dot_product += w[key] * x[key]\n",
    "        \n",
    "        if dot_product * label <= 0:\n",
    "            \n",
    "            for key, value in x.items():\n",
    "                if key != '*label*':\n",
    "                    if key in w:\n",
    "                        w[key] += label * x[key]\n",
    "                    else: w[key] = label * x[key]\n",
    "    \n",
    "    \n",
    "    # Second pass\n",
    "    random.shuffle(list_dict)\n",
    "    w_ret = dict(w) # Initilize w_ret\n",
    "    for x in list_dict:\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*':\n",
    "                if key in w:\n",
    "                    dot_product += w[key] * x[key]\n",
    "           \n",
    "        if dot_product * label <= 0:\n",
    "            for key, value in x.items():\n",
    "                if key != '*label*':\n",
    "                    if key in w:\n",
    "                        w[key] += label * x[key]\n",
    "                    else: w[key] = label * x[key]\n",
    "        \n",
    "        # Update w_ret\n",
    "        for key, value in w.items():\n",
    "            if key in w_ret:\n",
    "                w_ret[key] += value\n",
    "            else:\n",
    "                w_ret[key] = value\n",
    "\n",
    "                \n",
    "    length = len(list_dict) + 1\n",
    "    for key, value in w_ret.items():\n",
    "        w_ret[key] /= length\n",
    "    return w_ret\n",
    "    \n",
    "#     # Transfrom back to a vector\n",
    "#     w_vector = [0] * (len(vocabulary_dict) + 1)\n",
    "#     for key, value in w.items():\n",
    "#         if key in vocabulary_dict:\n",
    "#             w_vector[vocabulary_dict[key]] = value\n",
    "#     w_vector[-1] = w['*const*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_unigram(list_dict):\n",
    "    w = {} \n",
    "    \n",
    "    # First pass\n",
    "    random.shuffle(list_dict)\n",
    "    for x in list_dict:\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*':\n",
    "                if key in w:\n",
    "                    dot_product += w[key] * x[key]\n",
    "        \n",
    "        if dot_product * label <= 0:           \n",
    "            for key, value in x.items():\n",
    "                if key != '*label*':\n",
    "                    if key in w:\n",
    "                        w[key] += label * x[key]\n",
    "                    else: w[key] = label * x[key]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(list_dict)*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_dict_tmp = list_dict[:int(len(list_dict)*0.1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "w = train_unigram(list_dict_tmp) # Test on a smaller subset\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path2 = \"hw2data_1/reviews_te.csv\"\n",
    "df_test = pd.read_csv(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_unigram(df_test, w):\n",
    "    dict_list_test = data_compression(df_test)\n",
    "    count = 0\n",
    "    wrong = 0\n",
    "    for dictionary in dict_list_test:\n",
    "        count += 1\n",
    "        dot_product = 0\n",
    "        label = dictionary['*label*']\n",
    "        for key, value in dictionary.items():\n",
    "            if key in w and key != '*label*':\n",
    "                dot_product += w[key] * dictionary[key]\n",
    "        if dot_product * label <= 0: wrong += 1 \n",
    "    return (count - wrong) / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879384\n",
      "--- 145.81845998764038 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(test_unigram(df_test, w))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -190),\n",
       " ('mediocre', -169),\n",
       " ('bland', -135),\n",
       " ('terrible', -130),\n",
       " ('overpriced', -130),\n",
       " ('horrible', -129),\n",
       " ('disappointing', -128),\n",
       " ('ok', -127),\n",
       " ('awful', -119),\n",
       " ('poor', -118),\n",
       " ('dry', -112),\n",
       " ('disappointment', -109),\n",
       " ('excited', -103),\n",
       " ('meh', -102),\n",
       " ('rude', -100)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOP15 negative words\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('perfect', 136),\n",
       " ('amazing', 128),\n",
       " ('perfectly', 124),\n",
       " ('fantastic', 119),\n",
       " ('perfection', 107),\n",
       " ('delicious', 105),\n",
       " ('wonderful', 102),\n",
       " ('incredible', 98),\n",
       " ('excellent', 97),\n",
       " ('glad', 96),\n",
       " ('awesome', 91),\n",
       " ('pleased', 83),\n",
       " ('outstanding', 83),\n",
       " ('disappoint', 83),\n",
       " ('satisfied', 77)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOP15 Positive words\n",
    "sorted(w.items(), key=lambda x: x[1], reverse = True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF Representation\n",
    "frequency = {}\n",
    "for x in list_dict:\n",
    "    for key, value in x.items():\n",
    "        if key in frequency:\n",
    "            frequency[key] += 1\n",
    "        else:\n",
    "            frequency[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "list_dict1 = list(list_dict) # A shallow copy of list_dict\n",
    "# list_dict1 = copy.deepcopy(list_dict)  #This is a deep copy but takes up a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in list_dict1:\n",
    "    for key, value in x.items():\n",
    "        if key != '*label*' and key != '*const*':\n",
    "            x[key] = x[key] * math.log((len(list_dict1) / frequency[key]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "1000000   151064\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "w_tfidf = train_unigram(list_dict1, vocabulary_dict)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harman', -89.99999999999999),\n",
       " ('mediocre', -84.22474040199054),\n",
       " ('worst', -80.10731281700849),\n",
       " ('roum6b8yd4ykkugqcxtoug', -77.99999999999999),\n",
       " ('underwhelmed', -77.59042463214872),\n",
       " ('hopes', -71.74741110380765),\n",
       " ('disappointing', -70.60585455721528),\n",
       " ('not', -68.8592132041259),\n",
       " ('bland', -64.50013572257728),\n",
       " ('downhill', -63.45078877872636)]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(w_tfidf.items(), key=lambda x: x[1], reverse = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n00b', 92.83507630617008),\n",
       " ('great', 91.8086860288631),\n",
       " ('delicious', 84.33372264813823),\n",
       " ('ftr', 81.41181741504607),\n",
       " ('jmc', 80.96910013008055),\n",
       " ('and', 78.32414188144158),\n",
       " ('ohfh6alqqq35niebd1exuw', 77.99999999999999),\n",
       " ('perfection', 77.70343425406247),\n",
       " ('amazing', 74.67204708243527),\n",
       " ('zfpcpbzssimrybsg9jxndw', 71.99999999999999)]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(w_tfidf.items(), key=lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency['n00b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_tfidf(df_test, w_tfidf, frequency):\n",
    "    dict_list_test, dict_vocabulary_test = data_compression(df_test)\n",
    "    count = 0\n",
    "    wrong = 0\n",
    "    for x in dict_list_test:\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*' and key != '*const*':\n",
    "                if key in frequency: x[key] = x[key] * math.log((len(dict_list_test) / frequency[key]), 10)\n",
    "        \n",
    "        count += 1\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key in w and key != '*label*':\n",
    "                dot_product += w[key] * x[key]\n",
    "        if dot_product * label <= 0: wrong += 1 \n",
    "    return (count - wrong) / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.872884\n",
      "--- 283.20725202560425 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(test_tfidf(df_test, w_tfidf, frequency))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shallow copy and deep copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list1 = []\n",
    "dict1 = {'a': 1, 'b': 2}\n",
    "dict2 = {'f':1, 'e':2}\n",
    "list1.append(dict1)\n",
    "list1.append(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 3, 'b': 2}, {'f': 1, 'e': 2}]\n",
      "[{'a': 3, 'b': 2}, {'f': 1, 'e': 2}]\n"
     ]
    }
   ],
   "source": [
    "list2 = list(list1)\n",
    "list2[0]['a'] = 3\n",
    "print(list1)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 1, 'b': 2}, {'f': 1, 'e': 2}]\n",
      "[{'a': 3, 'b': 2}, {'f': 1, 'e': 2}]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "list2 = copy.deepcopy(list1)\n",
    "list2[0]['a'] = 3\n",
    "print(list1)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-268161e61baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Training set feature vectors after lifting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Training set labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "def train():\n",
    "    \n",
    "    X = [] # Training set feature vectors after lifting\n",
    "    Y = [] # Training set labels\n",
    "    w = [0] * len(X[0]) #weight vector\n",
    "    w = np.array(w)\n",
    "\n",
    "    # Need to shuffle the data\n",
    "    \n",
    "    # First pass\n",
    "    for i in range(len(X)):\n",
    "        if X[i] @ w * Y[i] <= 0: \n",
    "            w = w + Y[i] * X[i]\n",
    "\n",
    "    w_final = np.array(w)\n",
    "\n",
    "    # Need to shuffle the data\n",
    "\n",
    "    \n",
    "    # Second pass\n",
    "    for i in range(len(X)):\n",
    "        if X[i] @ w * Y[i] <= 0: \n",
    "            w = w + Y[i] * X[i]\n",
    "        w_final += w\n",
    "\n",
    "    w_final /= len(X) + 1\n",
    "    \n",
    "    return w_final\n",
    "\n",
    "def test(x, w):\n",
    "    if x @ w > 0: return 1\n",
    "    if x @ w < 0: return 0\n",
    "    else return -1  #Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem lies in dot product: it takes so much time for two vectors with 110000+ values\n",
    "list1 = [1] * 110000\n",
    "list2 = [i for i in range(110000)]  \n",
    "np.array(list1) @ np.array(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# != compares the value, whereas is not compares if they are the same object\n",
    "dict_test = {'*label*': 1}\n",
    "for key, value in dict_test.items():\n",
    "    if key != '*label*': print(\"haha\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
