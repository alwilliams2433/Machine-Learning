{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Implementation with Tf-idf Data Representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Member: \n",
    "1.Xingchen Zhou (xz2721)\n",
    "2.Lei You (ly2358)\n",
    "3.Jin Yan (jy2913)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"hw2data_1/reviews_tr.csv\"\n",
    "df_train = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path2 = \"hw2data_1/reviews_te.csv\"\n",
    "df_test = pd.read_csv(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')  # execute if you haven't downloaded nltk's stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words:  207429\n",
      "The number of words/features to exclude:  194762\n",
      "The number of feature to consider:  12488\n"
     ]
    }
   ],
   "source": [
    "# Feature Pruning\n",
    "vocabulary_dict = {}\n",
    "for text in df_train['text']:\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word not in vocabulary_dict: \n",
    "            vocabulary_dict[word] = 1\n",
    "        else:\n",
    "            vocabulary_dict[word] += 1\n",
    "print('The number of unique words: ', len(vocabulary_dict))\n",
    "\n",
    "low_occurrence_word_set = set()\n",
    "for key, value in vocabulary_dict.items():\n",
    "    if value < 150: low_occurrence_word_set.add(key)\n",
    "\n",
    "print('The number of words/features to exclude: ',len(low_occurrence_word_set))\n",
    "print('The number of feature to consider: ', len(vocabulary_dict) - len(low_occurrence_word_set) - len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Representation: tf-idf\n",
    "# Use hashmap to compress the data\n",
    "def data_compression(df_train):\n",
    "    list_dict = []  # Contains training examples compressed with hashmap\n",
    "\n",
    "    for index, row in df_train.iterrows():\n",
    "        new_dict = {}\n",
    "        words = row['text'] + \"\"\n",
    "        words = words.split()\n",
    "        for word in words:\n",
    "            # Feature Pruning: get rid of feature(word) that doesn't occur much\n",
    "            if word not in stopwords and word not in low_occurrence_word_set: \n",
    "                if word in new_dict:\n",
    "                    new_dict[word] += 1\n",
    "                else:\n",
    "                    new_dict[word] = 1\n",
    "\n",
    "        if(row['label'] == 1):  # Attach the label\n",
    "            new_dict['*label*'] = 1\n",
    "        else: new_dict['*label*'] = -1\n",
    "\n",
    "        new_dict['*const*'] = 1   # Lifting\n",
    "        list_dict.append(new_dict)\n",
    "\n",
    "    return list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_tfidf(list_dict):\n",
    "    w = {} \n",
    "    \n",
    "    # First pass\n",
    "    random.shuffle(list_dict)\n",
    "    for x in list_dict:\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*':\n",
    "                if key in w:\n",
    "                    dot_product += w[key] * x[key]\n",
    "        \n",
    "        if dot_product * label <= 0:           \n",
    "            for key, value in x.items():\n",
    "                if key != '*label*':\n",
    "                    if key in w:\n",
    "                        w[key] += label * x[key]\n",
    "                    else: w[key] = label * x[key]\n",
    "     \n",
    "    count = 0    # debug\n",
    "    start_time = time.time()    # debug\n",
    "    \n",
    "    # Second pass\n",
    "    random.shuffle(list_dict)\n",
    "    w_ret = dict(w) # Initilize w_ret\n",
    "    for x in list_dict:\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*':\n",
    "                if key in w:\n",
    "                    dot_product += w[key] * x[key]\n",
    "           \n",
    "        if dot_product * label <= 0:\n",
    "            for key, value in x.items():\n",
    "                if key != '*label*':\n",
    "                    if key in w:\n",
    "                        w[key] += label * x[key]\n",
    "                    else: w[key] = label * x[key]\n",
    "        \n",
    "        # Update w_ret\n",
    "        for key, value in w.items():\n",
    "            if key in w_ret:\n",
    "                w_ret[key] += value\n",
    "            else:\n",
    "                w_ret[key] = value\n",
    "        \n",
    "        # For debug purpose\n",
    "        if count % 10000 == 0:    \n",
    "            print('Current progress: ', count, end= ' ')     \n",
    "            print('     Time elapsed: ', time.time() - start_time)    \n",
    "        count += 1    \n",
    "    \n",
    "    # Calculate weighted weight vector\n",
    "    length = len(list_dict) + 1\n",
    "    for key, value in w_ret.items():\n",
    "        w_ret[key] /= length\n",
    "    return w_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 88.19826674461365 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Load features into a dict\n",
    "start_time = time.time()\n",
    "list_dict = data_compression(df_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify feature vectors' values based on tf-idf\n",
    "frequency = {}\n",
    "for x in list_dict:\n",
    "    for key, value in x.items():\n",
    "        if key in frequency:\n",
    "            frequency[key] += 1\n",
    "        else:\n",
    "            frequency[key] = 1\n",
    "\n",
    "for x in list_dict:\n",
    "    for key, value in x.items():\n",
    "        if key != '*label*' and key != '*const*':\n",
    "            x[key] = x[key] * math.log((len(list_dict) / frequency[key]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test weight vector w on the given testing set\n",
    "# dict_list_test is a dictionary storing testing examples \n",
    "# (Weights need to be adjusted! Uncomment the 4 lines to run this function by itself)\n",
    "# w is the learned weight vector\n",
    "def test_tfidf(dict_list_test, w):\n",
    "    count = 0\n",
    "    wrong = 0\n",
    "    for x in dict_list_test:\n",
    "#         for key, value in x.items():\n",
    "#             if key != '*label*' and key != '*const*':\n",
    "#                 # Ignore new words not seen in training set\n",
    "#                 if key in frequency: x[key] = x[key] * math.log((len(dict_list_test) / frequency[key]), 10)\n",
    "        \n",
    "        count += 1\n",
    "        dot_product = 0\n",
    "        label = x['*label*']\n",
    "        for key, value in x.items():\n",
    "            if key in w and key != '*label*':\n",
    "                dot_product += w[key] * x[key]\n",
    "        if dot_product * label <= 0: wrong += 1 \n",
    "    return (count - wrong) / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the performance of the classifier with different training sizes\n",
    "# start = 1, end = 5 <=> training size range: 10%, 20%,...,50%\n",
    "# list_dict: contains the entire training samples\n",
    "# Return two lists\n",
    "def test(list_dict, frequency, start = 1, end = 5):\n",
    "    w_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    # Load testing data to dictionary and adjust values by tf-idf\n",
    "    dict_list_test = data_compression(df_test)\n",
    "    for x in dict_list_test:\n",
    "        for key, value in x.items():\n",
    "            if key != '*label*' and key != '*const*':\n",
    "                # Ignore new words not seen in training set\n",
    "                if key in frequency: x[key] = x[key] * math.log((len(dict_list_test) / frequency[key]), 10)\n",
    "                    \n",
    "    for i in range(start, end + 1):\n",
    "        list_dict_current_size = list_dict[:int(len(list_dict) * i * 0.1)] # Training dict\n",
    "        w = train_tfidf(list_dict_current_size)\n",
    "        w_list.append(w)\n",
    "        print('Training size: ', i * 10, '%', end='')\n",
    "        \n",
    "        accuracy = test_tfidf(dict_list_test, w)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('   Accuracy: ', accuracy)\n",
    "        print()\n",
    "        \n",
    "    return w_list, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current progress:  0      Time elapsed:  0.5636146068572998\n",
      "Current progress:  10000      Time elapsed:  18.05366849899292\n",
      "Current progress:  20000      Time elapsed:  35.29811120033264\n",
      "Current progress:  30000      Time elapsed:  52.729758739471436\n",
      "Current progress:  40000      Time elapsed:  72.1111524105072\n",
      "Current progress:  50000      Time elapsed:  90.20764541625977\n",
      "Current progress:  60000      Time elapsed:  108.66355228424072\n",
      "Current progress:  70000      Time elapsed:  127.75526118278503\n",
      "Current progress:  80000      Time elapsed:  147.54956984519958\n",
      "Current progress:  90000      Time elapsed:  168.06379199028015\n",
      "Current progress:  100000      Time elapsed:  190.03611397743225\n",
      "Current progress:  110000      Time elapsed:  212.54300785064697\n",
      "Current progress:  120000      Time elapsed:  236.68603587150574\n",
      "Current progress:  130000      Time elapsed:  261.08786273002625\n",
      "Current progress:  140000      Time elapsed:  285.8650782108307\n",
      "Current progress:  150000      Time elapsed:  310.9178514480591\n",
      "Current progress:  160000      Time elapsed:  335.54885959625244\n",
      "Current progress:  170000      Time elapsed:  359.8104772567749\n",
      "Current progress:  180000      Time elapsed:  382.3368947505951\n",
      "Current progress:  190000      Time elapsed:  405.4390754699707\n",
      "Current progress:  200000      Time elapsed:  427.7957844734192\n",
      "Current progress:  210000      Time elapsed:  451.9594621658325\n",
      "Current progress:  220000      Time elapsed:  475.64967799186707\n",
      "Current progress:  230000      Time elapsed:  499.4791235923767\n",
      "Current progress:  240000      Time elapsed:  521.7851283550262\n",
      "Current progress:  250000      Time elapsed:  543.2710144519806\n",
      "Current progress:  260000      Time elapsed:  564.220053434372\n",
      "Current progress:  270000      Time elapsed:  584.8493626117706\n",
      "Current progress:  280000      Time elapsed:  605.0217859745026\n",
      "Current progress:  290000      Time elapsed:  625.0020949840546\n",
      "Current progress:  300000      Time elapsed:  644.4166135787964\n",
      "Current progress:  310000      Time elapsed:  663.5150191783905\n",
      "Current progress:  320000      Time elapsed:  682.5136840343475\n",
      "Current progress:  330000      Time elapsed:  701.5080988407135\n",
      "Current progress:  340000      Time elapsed:  720.2384762763977\n",
      "Current progress:  350000      Time elapsed:  738.5808680057526\n",
      "Current progress:  360000      Time elapsed:  757.0723528862\n",
      "Current progress:  370000      Time elapsed:  775.2896258831024\n",
      "Current progress:  380000      Time elapsed:  793.34024310112\n",
      "Current progress:  390000      Time elapsed:  811.1479849815369\n",
      "Current progress:  400000      Time elapsed:  828.9879343509674\n",
      "Current progress:  410000      Time elapsed:  846.7195229530334\n",
      "Current progress:  420000      Time elapsed:  864.4141707420349\n",
      "Current progress:  430000      Time elapsed:  882.0850176811218\n",
      "Current progress:  440000      Time elapsed:  899.6588568687439\n",
      "Current progress:  450000      Time elapsed:  917.3765964508057\n",
      "Current progress:  460000      Time elapsed:  935.1577684879303\n",
      "Current progress:  470000      Time elapsed:  952.954763174057\n",
      "Current progress:  480000      Time elapsed:  970.7981979846954\n",
      "Current progress:  490000      Time elapsed:  988.3940854072571\n",
      "Current progress:  500000      Time elapsed:  1006.1902978420258\n",
      "Current progress:  510000      Time elapsed:  1023.8450403213501\n",
      "Current progress:  520000      Time elapsed:  1041.6148014068604\n",
      "Current progress:  530000      Time elapsed:  1059.4814488887787\n",
      "Current progress:  540000      Time elapsed:  1077.3575117588043\n",
      "Current progress:  550000      Time elapsed:  1095.2615840435028\n",
      "Current progress:  560000      Time elapsed:  1112.9923527240753\n",
      "Current progress:  570000      Time elapsed:  1130.5952155590057\n",
      "Current progress:  580000      Time elapsed:  1148.197075843811\n",
      "Current progress:  590000      Time elapsed:  1165.7749388217926\n",
      "Training size:  60 %   Accuracy:  0.8672756011770513\n",
      "\n",
      "Current progress:  0      Time elapsed:  0.5928421020507812\n",
      "Current progress:  10000      Time elapsed:  17.26662588119507\n",
      "Current progress:  20000      Time elapsed:  34.1242151260376\n",
      "Current progress:  30000      Time elapsed:  51.29157042503357\n",
      "Current progress:  40000      Time elapsed:  68.45083999633789\n",
      "Current progress:  50000      Time elapsed:  86.27201700210571\n",
      "Current progress:  60000      Time elapsed:  104.0280122756958\n",
      "Current progress:  70000      Time elapsed:  122.23177766799927\n",
      "Current progress:  80000      Time elapsed:  140.19157338142395\n",
      "Current progress:  90000      Time elapsed:  158.15436935424805\n",
      "Current progress:  100000      Time elapsed:  176.1251664161682\n",
      "Current progress:  110000      Time elapsed:  194.09496307373047\n",
      "Current progress:  120000      Time elapsed:  212.0287561416626\n",
      "Current progress:  130000      Time elapsed:  229.99855303764343\n",
      "Current progress:  140000      Time elapsed:  247.9713499546051\n",
      "Current progress:  150000      Time elapsed:  265.962149143219\n",
      "Current progress:  160000      Time elapsed:  283.9249451160431\n",
      "Current progress:  170000      Time elapsed:  301.9017426967621\n",
      "Current progress:  180000      Time elapsed:  319.93454575538635\n",
      "Current progress:  190000      Time elapsed:  337.99535155296326\n",
      "Current progress:  200000      Time elapsed:  355.9901509284973\n",
      "Current progress:  210000      Time elapsed:  373.9699487686157\n",
      "Current progress:  220000      Time elapsed:  391.94674611091614\n",
      "Current progress:  230000      Time elapsed:  409.9205434322357\n",
      "Current progress:  240000      Time elapsed:  427.90634202957153\n",
      "Current progress:  250000      Time elapsed:  445.85613679885864\n",
      "Current progress:  260000      Time elapsed:  463.8419351577759\n",
      "Current progress:  270000      Time elapsed:  481.80173087120056\n",
      "Current progress:  280000      Time elapsed:  499.7635269165039\n",
      "Current progress:  290000      Time elapsed:  517.8933396339417\n",
      "Current progress:  300000      Time elapsed:  536.0431544780731\n",
      "Current progress:  310000      Time elapsed:  554.0209519863129\n",
      "Current progress:  320000      Time elapsed:  572.0967593193054\n",
      "Current progress:  330000      Time elapsed:  590.4665961265564\n",
      "Current progress:  340000      Time elapsed:  608.6674160957336\n",
      "Current progress:  350000      Time elapsed:  626.8832376003265\n",
      "Current progress:  360000      Time elapsed:  644.9880478382111\n",
      "Current progress:  370000      Time elapsed:  663.1818671226501\n",
      "Current progress:  380000      Time elapsed:  681.2806766033173\n",
      "Current progress:  390000      Time elapsed:  699.4144899845123\n",
      "Current progress:  400000      Time elapsed:  717.5583040714264\n",
      "Current progress:  410000      Time elapsed:  735.702118396759\n",
      "Current progress:  420000      Time elapsed:  753.7659246921539\n",
      "Current progress:  430000      Time elapsed:  771.9017379283905\n",
      "Current progress:  440000      Time elapsed:  790.0025477409363\n",
      "Current progress:  450000      Time elapsed:  808.119359254837\n",
      "Current progress:  460000      Time elapsed:  826.2341706752777\n",
      "Current progress:  470000      Time elapsed:  844.2769746780396\n",
      "Current progress:  480000      Time elapsed:  862.4387907981873\n",
      "Current progress:  490000      Time elapsed:  880.445591211319\n",
      "Current progress:  500000      Time elapsed:  898.4683933258057\n",
      "Current progress:  510000      Time elapsed:  916.5341999530792\n",
      "Current progress:  520000      Time elapsed:  934.5620024204254\n",
      "Current progress:  530000      Time elapsed:  952.6768136024475\n",
      "Current progress:  540000      Time elapsed:  970.7226181030273\n",
      "Current progress:  550000      Time elapsed:  988.7224180698395\n",
      "Current progress:  560000      Time elapsed:  1006.7212176322937\n",
      "Current progress:  570000      Time elapsed:  1024.738018989563\n",
      "Current progress:  580000      Time elapsed:  1042.7088158130646\n",
      "Current progress:  590000      Time elapsed:  1060.824627161026\n",
      "Current progress:  600000      Time elapsed:  1078.8054251670837\n",
      "Current progress:  610000      Time elapsed:  1096.7872233390808\n",
      "Current progress:  620000      Time elapsed:  1114.7870230674744\n",
      "Current progress:  630000      Time elapsed:  1132.8228266239166\n",
      "Current progress:  640000      Time elapsed:  1150.8926334381104\n",
      "Current progress:  650000      Time elapsed:  1169.0104448795319\n",
      "Current progress:  660000      Time elapsed:  1187.0302467346191\n",
      "Current progress:  670000      Time elapsed:  1205.0290462970734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current progress:  680000      Time elapsed:  1223.0198452472687\n",
      "Current progress:  690000      Time elapsed:  1241.0226452350616\n",
      "Training size:  70 %   Accuracy:  0.86923735325907\n",
      "\n",
      "Current progress:  0      Time elapsed:  0.6870689392089844\n",
      "Current progress:  10000      Time elapsed:  18.232823133468628\n",
      "Current progress:  20000      Time elapsed:  36.03360295295715\n",
      "Current progress:  30000      Time elapsed:  54.016401052474976\n",
      "Current progress:  40000      Time elapsed:  72.040203332901\n",
      "Current progress:  50000      Time elapsed:  90.17801690101624\n",
      "Current progress:  60000      Time elapsed:  108.39183807373047\n",
      "Current progress:  70000      Time elapsed:  126.64166283607483\n",
      "Current progress:  80000      Time elapsed:  144.91749024391174\n",
      "Current progress:  90000      Time elapsed:  163.19931840896606\n",
      "Current progress:  100000      Time elapsed:  181.57515573501587\n",
      "Current progress:  110000      Time elapsed:  199.94299221038818\n",
      "Current progress:  120000      Time elapsed:  218.32383012771606\n",
      "Current progress:  130000      Time elapsed:  236.6766653060913\n",
      "Current progress:  140000      Time elapsed:  255.09150671958923\n",
      "Current progress:  150000      Time elapsed:  273.4773449897766\n",
      "Current progress:  160000      Time elapsed:  291.5919551849365\n",
      "Current progress:  170000      Time elapsed:  310.1821303367615\n",
      "Current progress:  180000      Time elapsed:  329.0600299835205\n",
      "Current progress:  190000      Time elapsed:  347.7418978214264\n",
      "Current progress:  200000      Time elapsed:  366.3627598285675\n",
      "Current progress:  210000      Time elapsed:  385.12263560295105\n",
      "Current progress:  220000      Time elapsed:  403.7955026626587\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b43aa09420e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0maccuracy_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mw_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-29d9ec477f9a>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(list_dict, frequency, start, end)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mlist_dict_current_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_dict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Training dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_dict_current_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mw_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training size: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-880d96a81b2e>\u001b[0m in \u001b[0;36mtrain_tfidf\u001b[1;34m(list_dict)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw_ret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mw_ret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mw_ret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_list = []\n",
    "accuracy_list = []\n",
    "w_list, accuracy_list = test(list_dict, frequency, 6, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training size:  10 %   Accuracy:  0.8416822336484215\n",
    "    Training size:  20 %   Accuracy:  0.8529216986024079\n",
    "    Training size:  30 %   Accuracy:  0.8600096213318672\n",
    "    Training size:  40 %   Accuracy:  0.8615808972829109\n",
    "    Training size:  50 %   Accuracy:  0.864242382591637\n",
    "    Training size:  60 %   Accuracy:  0.8672756011770513\n",
    "    Training size:  70 %   Accuracy:  0.86923735325907\n",
    "    Training size:  100 %   Accuracy:  0.87349822880027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -71.7436404870189),\n",
       " ('mediocre', -68.47342152673993),\n",
       " ('ok', -62.393427437900705),\n",
       " ('underwhelmed', -58.650900798881246),\n",
       " ('bland', -58.33322102556888),\n",
       " ('lacked', -55.47041422989345),\n",
       " ('horrible', -54.98175220586404),\n",
       " ('meh', -54.440690248503344),\n",
       " ('disappointing', -52.77887329955581),\n",
       " ('redeeming', -50.89955179151541),\n",
       " ('terrible', -49.997257831667454),\n",
       " ('inedible', -49.10891320690858),\n",
       " ('flavorless', -48.168304744970946),\n",
       " ('okay', -47.262501384207965),\n",
       " ('awful', -47.068092068339325),\n",
       " ('poisoning', -46.63904997858717),\n",
       " ('subpar', -46.48641391156677),\n",
       " ('underwhelming', -46.4596475483425),\n",
       " ('disgrace', -45.99361021482247),\n",
       " ('uninspired', -45.0773884316419)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top20 negative words when training size is 100%\n",
    "sorted(w_list[0].items(), key=lambda x: x[1], reverse = False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delicious', 77.37252528943662),\n",
       " ('great', 69.49027411617178),\n",
       " ('amazing', 62.85549081894239),\n",
       " ('gem', 57.92438642331946),\n",
       " ('excellent', 56.94612610720251),\n",
       " ('perfect', 55.1379905178235),\n",
       " ('awesome', 54.675440407079705),\n",
       " ('perfection', 54.06078936326045),\n",
       " ('exceeded', 52.42922851682227),\n",
       " ('fantastic', 49.499153319813125),\n",
       " ('best', 48.40850019896811),\n",
       " ('perfectly', 47.25770572958858),\n",
       " ('incredible', 45.924191747681554),\n",
       " ('hooray', 41.61498934980593),\n",
       " ('heartbeat', 40.90991327133111),\n",
       " ('heaven', 38.94621224504956),\n",
       " ('magnificent', 38.665487933978156),\n",
       " ('addicted', 38.32989280591459),\n",
       " ('wonderful', 38.09203936915797),\n",
       " ('mazing', 37.97021233295175)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top20 positive words when training size is 100%\n",
    "sorted(w_list[0].items(), key=lambda x: x[1], reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this block only for a specific training size test\n",
    "k = 0.05 # training size percentage e.g. 0.5 <=> 50%\n",
    "list_dict_tmp = list_dict[:int(len(list_dict) * k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current progress:  0  time elapsed:  0.0\n",
      "Total Traing Time: --- 12.404083013534546 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Run this block for a specific training size test\n",
    "start_time = time.time()\n",
    "w = train_tfidf(list_dict_tmp) # Test on a smaller subset first\n",
    "print(\"Traing Time: --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8089978195812846\n",
      "Testing Time: --- 44.49867010116577 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Run this block for a specific training size test\n",
    "start_time = time.time()\n",
    "print(test_tfidf(df_test, w, frequency))\n",
    "print(\"Testing Time: --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean1  250.0\n",
      "std1  111.80339887498948\n",
      "\n",
      "mean2  2.5\n",
      "std1  1.118033988749895\n",
      "\n",
      "Feature 1 before scale: [100 200 300 400]\n",
      "afater scale [ 0.89442719  1.78885438  2.68328157  3.57770876]\n",
      "\n",
      "Feature 2 before scale: [[1 2 3 4]]\n",
      "afater scale [[ 0.89442719  1.78885438  2.68328157  3.57770876]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "array1 = np.array([100,200,300,400])\n",
    "array2 = np.array([[1,2,3,4]])\n",
    "print('mean1 ', np.mean(array1))\n",
    "print('std1 ', math.sqrt(np.var(array1)))\n",
    "print()\n",
    "print('mean2 ', np.mean(array2))\n",
    "print('std1 ', math.sqrt(np.var(array2)))\n",
    "print()\n",
    "print('Feature 1 before scale:', array1)\n",
    "array1_ = np.divide(array1, math.sqrt(np.var(array1))) # Scaling\n",
    "print('afater scale', array1_)\n",
    "print()\n",
    "print('Feature 2 before scale:', array2)\n",
    "array2_ =  np.divide(array2, math.sqrt(np.var(array2)))\n",
    "print('afater scale',array2_)\n",
    "print(math.sqrt(np.var(array2_)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
